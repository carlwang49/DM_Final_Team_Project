{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install & imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U xgboost -f /kaggle/input/xgboost-python-package/ --no-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#General\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Modeling\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import torch\n",
    "\n",
    "# Geolocation\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "# Compressed serialization\n",
    "from joblib import dump, load\n",
    "\n",
    "# Options\n",
    "pd.set_option('display.max_columns', 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"predict-energy-behavior-of-prosumers/\"\n",
    "\n",
    "# Read CSVs and parse relevant date columns\n",
    "train = pd.read_csv(DATA_DIR + \"train.csv\")\n",
    "client = pd.read_csv(DATA_DIR + \"client.csv\")\n",
    "historical_weather = pd.read_csv(DATA_DIR + \"historical_weather.csv\")\n",
    "forecast_weather = pd.read_csv(DATA_DIR + \"forecast_weather.csv\")\n",
    "electricity = pd.read_csv(DATA_DIR + \"electricity_prices.csv\")\n",
    "gas = pd.read_csv(DATA_DIR + \"gas_prices.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False # False/True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU or CPU use for model\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def display_df(df, name):\n",
    "    '''Display df shape and first 5 rows '''\n",
    "    print(f'{name} data has {df.shape[0]} rows and {df.shape[1]} columns.')\n",
    "    display(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location from https://www.kaggle.com/datasets/michaelo/fabiendaniels-mapping-locations-and-county-codes/data\n",
    "# location = (pd.read_csv(\"/kaggle/input/fabiendaniels-mapping-locations-and-county-codes/county_lon_lats.csv\")\n",
    "#             .drop(columns = [\"Unnamed: 0\"])\n",
    "#            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_df(train, 'train')\n",
    "display_df(client, 'client')\n",
    "display_df(historical_weather, 'historical weather')\n",
    "display_df(forecast_weather, 'forecast weather')\n",
    "display_df(electricity, 'electricity prices')\n",
    "display_df(gas, 'gas prices')\n",
    "# display_df(location, 'location data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See county codes\n",
    "with open(DATA_DIR + 'county_id_to_name_map.json') as f:\n",
    "    county_codes = json.load(f)\n",
    "pd.DataFrame(county_codes, index=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    train[train['is_consumption']==0]\n",
    "    .target.describe(\n",
    "        percentiles = [0, 0.001, 0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99, 0.999]\n",
    "    )\n",
    ").round(2).T\n",
    "\n",
    "# 從train數據集中選取is_consumption列等於0的所有行\n",
    "# describe函數默認計算數據的count（計數）、mean（平均值）、std（標準差）、min（最小值）、25%、50%、75%百分位數和max（最大值）\n",
    "# percentiles參數指定了更多的百分位數來計算\n",
    "# .T表示對DataFrame進行轉置，使列變成行，行變成列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(train[train['is_consumption']==1].target.describe(percentiles = [0, 0.001, 0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99, 0.999])).round(2).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class FeatureProcessorClass():\n",
    "    def __init__(self):         \n",
    "        # Columns to join on for the different datasets\n",
    "#         self.weather_join = ['datetime', 'county', 'data_block_id']\n",
    "        self.weather_join = ['datetime', 'data_block_id']\n",
    "        self.gas_join = ['data_block_id']\n",
    "        self.electricity_join = ['datetime', 'data_block_id']\n",
    "        self.client_join = ['county', 'is_business', 'product_type', 'data_block_id']\n",
    "        \n",
    "        # Columns of latitude & longitude 經緯度\n",
    "        self.lat_lon_columns = ['latitude', 'longitude']\n",
    "        \n",
    "        # Aggregate stats 聚合統計函數\n",
    "        self.agg_stats = ['mean'] #, 'min', 'max', 'std', 'median']\n",
    "        \n",
    "        # Categorical columns (specify for XGBoost) 指定了哪些列是分類型列\n",
    "        # 使用XGBoost等機器學習算法時非常重要，因為這些算法需要知道哪些列是分類型數據\n",
    "        self.category_columns = ['county', 'is_business', 'product_type', 'is_consumption', 'data_block_id']\n",
    "\n",
    "    def create_new_column_names(self, df, suffix, columns_no_change):\n",
    "        '''Change column names by given suffix, keep columns_no_change, and return back the data'''\n",
    "        df.columns = [col + suffix \n",
    "                      if col not in columns_no_change\n",
    "                      else col\n",
    "                      for col in df.columns\n",
    "                      ]\n",
    "        return df \n",
    "\n",
    "    def flatten_multi_index_columns(self, df):\n",
    "        df.columns = ['_'.join([col for col in multi_col if len(col)>0]) \n",
    "                      for multi_col in df.columns]\n",
    "        return df\n",
    "    \n",
    "    def create_data_features(self, data):\n",
    "        '''Create features for main data (test or train) set'''\n",
    "        # To datetime\n",
    "        data['datetime'] = pd.to_datetime(data['datetime'])\n",
    "        \n",
    "        # Time period features\n",
    "        data['date'] = data['datetime'].dt.normalize()\n",
    "        data['year'] = data['datetime'].dt.year\n",
    "        data['quarter'] = data['datetime'].dt.quarter\n",
    "        data['month'] = data['datetime'].dt.month\n",
    "        data['week'] = data['datetime'].dt.isocalendar().week\n",
    "        data['hour'] = data['datetime'].dt.hour\n",
    "        \n",
    "        # Day features\n",
    "        data['day_of_year'] = data['datetime'].dt.day_of_year\n",
    "        data['day_of_month']  = data['datetime'].dt.day\n",
    "        data['day_of_week'] = data['datetime'].dt.day_of_week\n",
    "        return data\n",
    "\n",
    "    def create_client_features(self, client):\n",
    "        '''Create client features'''\n",
    "        # Modify column names - specify suffix\n",
    "        client = self.create_new_column_names(client, \n",
    "                                           suffix='_client',\n",
    "                                           columns_no_change = self.client_join\n",
    "                                          )       \n",
    "        return client\n",
    "    \n",
    "    def create_historical_weather_features(self, historical_weather):\n",
    "        '''Create historical weather features'''\n",
    "        \n",
    "        # To datetime\n",
    "        historical_weather['datetime'] = pd.to_datetime(historical_weather['datetime'])\n",
    "        \n",
    "        # Add county\n",
    "        historical_weather[self.lat_lon_columns] = historical_weather[self.lat_lon_columns].astype(float).round(1)\n",
    "#         historical_weather = historical_weather.merge(location, how = 'left', on = self.lat_lon_columns)\n",
    "\n",
    "        # Modify column names - specify suffix\n",
    "        historical_weather = self.create_new_column_names(historical_weather,\n",
    "                                                          suffix='_h',\n",
    "                                                          columns_no_change = self.lat_lon_columns + self.weather_join\n",
    "                                                          ) \n",
    "        \n",
    "        # Group by & calculate aggregate stats \n",
    "        agg_columns = [col for col in historical_weather.columns if col not in self.lat_lon_columns + self.weather_join]\n",
    "        agg_dict = {agg_col: self.agg_stats for agg_col in agg_columns}\n",
    "        historical_weather = historical_weather.groupby(self.weather_join).agg(agg_dict).reset_index() \n",
    "        \n",
    "        # Flatten the multi column aggregates\n",
    "        historical_weather = self.flatten_multi_index_columns(historical_weather) \n",
    "        \n",
    "        # Test set has 1 day offset for hour<11 and 2 day offset for hour>11\n",
    "        historical_weather['hour_h'] = historical_weather['datetime'].dt.hour\n",
    "        historical_weather['datetime'] = (historical_weather\n",
    "                                               .apply(lambda x: \n",
    "                                                      x['datetime'] + pd.DateOffset(1) \n",
    "                                                      if x['hour_h']< 11 \n",
    "                                                      else x['datetime'] + pd.DateOffset(2),\n",
    "                                                      axis=1)\n",
    "                                              )\n",
    "        \n",
    "        return historical_weather\n",
    "    \n",
    "    def create_forecast_weather_features(self, forecast_weather):\n",
    "        '''Create forecast weather features'''\n",
    "        \n",
    "        # Rename column and drop\n",
    "        forecast_weather = (forecast_weather\n",
    "                            .rename(columns = {'forecast_datetime': 'datetime'})\n",
    "                            .drop(columns = 'origin_datetime') # not needed\n",
    "                           )\n",
    "        \n",
    "        # To datetime\n",
    "        forecast_weather['datetime'] = (pd.to_datetime(forecast_weather['datetime'])\n",
    "                                        .dt\n",
    "                                        .tz_convert('Europe/Brussels') # change to different time zone?\n",
    "                                        .dt\n",
    "                                        .tz_localize(None)\n",
    "                                       )\n",
    "\n",
    "        # Add county\n",
    "        forecast_weather[self.lat_lon_columns] = forecast_weather[self.lat_lon_columns].astype(float).round(1)\n",
    "#         forecast_weather = forecast_weather.merge(location, how = 'left', on = self.lat_lon_columns)\n",
    "        \n",
    "        # Modify column names - specify suffix\n",
    "        forecast_weather = self.create_new_column_names(forecast_weather,\n",
    "                                                        suffix='_f',\n",
    "                                                        columns_no_change = self.lat_lon_columns + self.weather_join\n",
    "                                                        ) \n",
    "        \n",
    "        # Group by & calculate aggregate stats \n",
    "        agg_columns = [col for col in forecast_weather.columns if col not in self.lat_lon_columns + self.weather_join]\n",
    "        agg_dict = {agg_col: self.agg_stats for agg_col in agg_columns}\n",
    "        forecast_weather = forecast_weather.groupby(self.weather_join).agg(agg_dict).reset_index() \n",
    "        \n",
    "        # Flatten the multi column aggregates\n",
    "        forecast_weather = self.flatten_multi_index_columns(forecast_weather)     \n",
    "        return forecast_weather\n",
    "\n",
    "    def create_electricity_features(self, electricity):\n",
    "        '''Create electricity prices features'''\n",
    "        # To datetime\n",
    "        electricity['forecast_date'] = pd.to_datetime(electricity['forecast_date'])\n",
    "        \n",
    "        # Test set has 1 day offset\n",
    "        electricity['datetime'] = electricity['forecast_date'] + pd.DateOffset(1)\n",
    "        \n",
    "        # Modify column names - specify suffix\n",
    "        electricity = self.create_new_column_names(electricity, \n",
    "                                                   suffix='_electricity',\n",
    "                                                   columns_no_change = self.electricity_join\n",
    "                                                  )             \n",
    "        return electricity\n",
    "\n",
    "    def create_gas_features(self, gas):\n",
    "        ''' Create gas prices features'''\n",
    "        # Mean gas price\n",
    "        gas['mean_price_per_mwh'] = (gas['lowest_price_per_mwh'] + gas['highest_price_per_mwh'])/2\n",
    "        \n",
    "        # Modify column names - specify suffix\n",
    "        gas = self.create_new_column_names(gas, \n",
    "                                           suffix='_gas',\n",
    "                                           columns_no_change = self.gas_join\n",
    "                                          )       \n",
    "        return gas\n",
    "    \n",
    "    def __call__(self, data, client, historical_weather, forecast_weather, electricity, gas):\n",
    "        '''Processing of features from all datasets, merge together and return features for dataframe df '''\n",
    "        # Create features for relevant dataset\n",
    "        data = self.create_data_features(data)\n",
    "        client = self.create_client_features(client)\n",
    "        historical_weather = self.create_historical_weather_features(historical_weather)\n",
    "        forecast_weather = self.create_forecast_weather_features(forecast_weather)\n",
    "        electricity = self.create_electricity_features(electricity)\n",
    "        gas = self.create_gas_features(gas)\n",
    "        \n",
    "        # Merge all datasets into one df\n",
    "        df = data.merge(client, how='left', on = self.client_join)\n",
    "        df = df.merge(historical_weather, how='left', on = self.weather_join)\n",
    "        df = df.merge(forecast_weather, how='left', on = self.weather_join)\n",
    "        df = df.merge(electricity, how='left', on = self.electricity_join)\n",
    "        df = df.merge(gas, how='left', on = self.gas_join)\n",
    "        \n",
    "        # Change columns to categorical for XGBoost\n",
    "        df[self.category_columns] = df[self.category_columns].astype('category')\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在時間序列數據集中為每個觀測添加過去特定天數的目標值\n",
    "# 這種方法在時間序列分析或預測任務中很常見，尤其是當預測的目標值依賴於過去的值時\n",
    "# data（原始數據集）和N_day_lags（要創建的天數滯後數）\n",
    "def create_revealed_targets_train(data, N_day_lags):\n",
    "    '''Create past revealed_targets for train set based on number of day lags N_day_lags'''    \n",
    "    original_datetime = data['datetime']\n",
    "    revealed_targets = data[['datetime', 'prediction_unit_id', 'is_consumption', 'target']].copy() # 新的 DataFrame 框架\n",
    "    \n",
    "    # Create revealed targets for all day lags\n",
    "    # 通過迴圈，從2天滯後開始，直到N_day_lags（包含）\n",
    "    for day_lag in range(2, N_day_lags+1):\n",
    "        revealed_targets['datetime'] = original_datetime + pd.DateOffset(day_lag)\n",
    "        # 使用merge函數將更新後的revealed_targets與原始數據集合併，基於 datetime、prediction_unit_id 和 is_consumption 列\n",
    "        data = data.merge(revealed_targets, \n",
    "                          how='left', \n",
    "                          on = ['datetime', 'prediction_unit_id', 'is_consumption'],\n",
    "                          suffixes = ('', f'_{day_lag}_days_ago') # 合併後的列添加後綴\n",
    "                         )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用於測量並顯示整個單元格執行所需的時間\n",
    "# (Jupyter Notebook的魔法命令?)\n",
    "# %%time\n",
    "# Create all features\n",
    "# 表示在特徵工程中將考慮過去15天的數據\n",
    "N_day_lags = 15 # Specify how many days we want to go back (at least 2)\n",
    "\n",
    "FeatureProcessor = FeatureProcessorClass()\n",
    "\n",
    "data = FeatureProcessor(data = train.copy(),\n",
    "                      client = client.copy(),\n",
    "                      historical_weather = historical_weather.copy(),\n",
    "                      forecast_weather = forecast_weather.copy(),\n",
    "                      electricity = electricity.copy(),\n",
    "                      gas = gas.copy(),\n",
    "                     )\n",
    "\n",
    "df = create_revealed_targets_train(data.copy(), \n",
    "                                  N_day_lags = N_day_lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost single fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DM_Final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
